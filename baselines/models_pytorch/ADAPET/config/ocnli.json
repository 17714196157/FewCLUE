{
    "pretrained_weight":  "hfl/chinese-roberta-wwm-ext",
    "dataset": "ocnli",
    "max_text_length": 256,
    "batch_size": 1,
    "eval_batch_size": 32,
    "num_batches": 500,
    "max_num_lbl_tok": 1,
    "eval_every": 8,
    "warmup_ratio": 0.06,
    "mask_alpha": 0.105,
    "grad_accumulation_factor": 16,
    "dropout_rate": 0.1,
    "seed": 42,
    "lr": 1e-5,
    "weight_decay": 1e-2,
    "pattern": 1,
    "eval_train": true
}
